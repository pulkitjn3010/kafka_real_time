networks:
  spark-net:
    driver: bridge

volumes:
  zookeeper_data:
  zookeeper_log:
  kafka_data_1:
  kafka_data_2:
  clickhouse_data:
  delta_lake_data:

services:
  # 1. Zookeeper (Kafka's dependency)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    networks:
      - spark-net
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log

  # 2. Kafka Broker-1
  kafka-1:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-1
    networks:
      - spark-net
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,INTERNAL://kafka-1:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
    depends_on:
      - zookeeper
    volumes:
      - kafka_data_1:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server kafka-1:9093 --list || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 15

  # 3. Kafka Broker-2
  kafka-2:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-2
    networks:
      - spark-net
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094,INTERNAL://kafka-2:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
    depends_on:
      - zookeeper
    volumes:
      - kafka_data_2:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server kafka-2:9093 --list || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 15

  # 4. Kafka Setup (Create Topics)
  kafka-setup:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-setup
    networks:
      - spark-net
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for Kafka cluster...';
        cub kafka-ready -b kafka-1:9093,kafka-2:9093 2 60;
        echo '✓ Kafka cluster is ready!';
        
        echo 'Creating topic: raw_events';
        kafka-topics --create --if-not-exists --topic raw_events --bootstrap-server kafka-1:9093,kafka-2:9093 --partitions 4 --replication-factor 2;
        
        echo 'Creating topic: processed_events';
        kafka-topics --create --if-not-exists --topic processed_events --bootstrap-server kafka-1:9093,kafka-2:9093 --partitions 4 --replication-factor 2;
        
        echo 'Waiting 20 seconds for topic replication...';
        sleep 20;
        
        echo 'Verifying topics...';
        kafka-topics --describe --topic raw_events --bootstrap-server kafka-1:9093,kafka-2:9093;
        kafka-topics --describe --topic processed_events --bootstrap-server kafka-1:9093,kafka-2:9093;
        
        echo '✓ All topics are ready and replicated!';
      "
    restart: on-failure

  # 5. ClickHouse Database
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    networks:
      - spark-net
    ports:
      - "8123:8123"
      - "9000:9000"
    environment:
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=password
    volumes:
      - clickhouse_data:/var/lib/clickhouse/
    healthcheck:
      test: ["CMD", "clickhouse-client", "--user", "default", "--password", "password", "-q", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  # 6. ClickHouse Setup
  clickhouse-setup:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse-setup
    networks:
      - spark-net
    depends_on:
      clickhouse:
        condition: service_healthy
      kafka-setup:
        condition: service_completed_successfully
    command: >
      bash -c "
        echo 'Waiting for ClickHouse...';
        until clickhouse-client --host clickhouse --user default --password password -q 'SELECT 1'; do
          sleep 2;
        done;
        
        echo 'Creating ClickHouse database...';
        clickhouse-client --host clickhouse --user default --password password -q 'CREATE DATABASE IF NOT EXISTS spark_db';
        
        echo 'Creating main events table...';
        clickhouse-client --host clickhouse --user default --password password -q \"
          CREATE TABLE IF NOT EXISTS spark_db.events_table (
            user_id UInt64,
            event String,
            timestamp DateTime
          ) ENGINE = MergeTree()
          ORDER BY (timestamp, user_id)
        \";
        
        echo 'Creating Kafka engine table...';
        clickhouse-client --host clickhouse --user default --password password -q \"
          CREATE TABLE IF NOT EXISTS spark_db.events_kafka_pipe (
            user_id UInt64,
            event String,
            timestamp DateTime
          ) ENGINE = Kafka
          SETTINGS
              kafka_broker_list = 'kafka-1:9093,kafka-2:9093',
              kafka_topic_list = 'processed_events',
              kafka_group_name = 'clickhouse_consumers_1',
              kafka_format = 'JSONEachRow',
              kafka_num_consumers = 2
        \";
        
        echo 'Creating materialized view...';
        clickhouse-client --host clickhouse --user default --password password -q \"
          CREATE MATERIALIZED VIEW IF NOT EXISTS spark_db.events_mv
          TO spark_db.events_table
          AS SELECT 
            user_id,
            event,
            parseDateTimeBestEffort(toString(timestamp)) as timestamp
          FROM spark_db.events_kafka_pipe
        \";
        
        echo 'ClickHouse setup complete!';
      "
    restart: on-failure

  # 7. Spark Job
  spark-job:
    container_name: spark-job
    build: .
    networks:
      - spark-net
    depends_on:
      kafka-setup:
        condition: service_completed_successfully
      clickhouse-setup:
        condition: service_completed_successfully
    volumes:
      - delta_lake_data:/delta-lake
    command:
      - "/opt/spark/bin/spark-submit"
      - "--master"
      - "local[*]"
      - "--packages"
      - "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,io.delta:delta-spark_2.12:3.2.0"
      - "--conf"
      - "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"
      - "--conf"
      - "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"
      - "/opt/spark/work-dir/spark_consumer.py"

  # 8. Spark Shell (for debugging)
  spark-shell:
    container_name: spark-shell
    build: .
    networks:
      - spark-net
    volumes:
      - delta_lake_data:/delta-lake
    depends_on:
      - spark-job
    command: tail -f /dev/null